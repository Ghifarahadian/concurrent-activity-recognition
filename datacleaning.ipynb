{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datacleaning.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghifarahadian/concurrent-activity-recognition/blob/master/datacleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPHzniw2TX_4"
      },
      "source": [
        "# this is a classic usage for google colab\n",
        "\n",
        "import sys\n",
        "import os.path as osp\n",
        "import os\n",
        "import gc\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT = osp.join('/content', 'drive', 'My Drive', 'concurrent-activity-recognition')\n",
        "os.chdir(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvRzpABgTp-w"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTZndHicTwiv"
      },
      "source": [
        "# data column constant\n",
        "USE_FEATURES = \"all\"\n",
        "DATASET_CLEANED_DIR = \"dataset_cleaned_all_features\"\n",
        "DATASET_PARSED_DIR = \"dataset_parsed_all_features_label1\"\n",
        "DATA_TYPES = [\"train\", \"test\"]\n",
        "\n",
        "if USE_FEATURES == \"default\":\n",
        "    # This is the list of features that are retrievable from\n",
        "    # fitbit and phone\n",
        "    col_right_wrist = [22, 23, 24]\n",
        "    col_right_wrist_name = [\"right_wrist_x\", \"right_wrist_y\", \"right_wrist_z\"]\n",
        "    col_left_wrist = [31, 32, 33]\n",
        "    col_left_wrist_name = [\"left_wrist_x\", \"left_wrist_y\", \"left_wrist_z\"]\n",
        "    col_right_hand = [34, 35, 36]\n",
        "    col_right_hand_name = [\"right_hand_x\", \"right_hand_y\", \"right_hand_z\"]\n",
        "    col_left_hand = [13, 14, 15]\n",
        "    col_left_hand_name = [\"left_hand_x\", \"left_hand_y\", \"left_hand_z\"]\n",
        "    col_hip = [4, 5, 6]\n",
        "    col_hip_name = [\"hip_x\", \"hip_y\", \"hip_z\"]\n",
        "    col_label = [243, 247, 249]\n",
        "    col_label_name = [\"Locomotion\", \"LL_Right_Arm\", \"ML_Both_Arms\"]\n",
        "\n",
        "    col_all = []\n",
        "    for col in [col_hip, col_left_hand, col_right_wrist, col_left_wrist,\n",
        "                col_right_hand, col_label]:\n",
        "        col_all.extend(col)\n",
        "\n",
        "    col_all_name = []\n",
        "    for col in [col_hip_name, col_left_hand_name, col_right_wrist_name,\n",
        "                col_left_wrist_name, col_right_hand_name, col_label_name]:\n",
        "        col_all_name.extend(col)\n",
        "\n",
        "    col_feature_name = []\n",
        "    for col in [col_hip_name, col_left_hand_name, col_right_wrist_name,\n",
        "                col_left_wrist_name, col_right_hand_name]:\n",
        "        col_feature_name.extend(col)\n",
        "elif USE_FEATURES == \"all\":\n",
        "    # This is the list of all accelerometer data from sensors placed on body\n",
        "    col_all = [i for i in range(1, 37)]\n",
        "    col_all.extend([243, 247, 249])\n",
        "\n",
        "    feature_list = [\"RKNUp\", \"Hip\", \"LUAUp\", \"RUADn\", \"LH\", \"Back\", \"RKNDn\",\n",
        "                    \"RWR\", \"RUAUp\", \"LUADn\", \"LWR\", \"RH\"]\n",
        "    axis_list = [\"X\", \"Y\", \"Z\"]\n",
        "    col_feature_name = [f + \"_\" + a\n",
        "                        for f in feature_list\n",
        "                        for a in axis_list]\n",
        "\n",
        "    col_all_name = col_feature_name.copy()\n",
        "    col_all_name.extend([\"Locomotion\", \"LL_Right_Arm\", \"ML_Both_Arms\"])\n",
        "else:\n",
        "    raise Exception(\"USE_FEATURES is not recognized\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6NeygvZVD4A"
      },
      "source": [
        "# clean the data, both train data and test data\n",
        "\n",
        "# normalizing function\n",
        "def normalize(ser):\n",
        "    mean = ser.mean()\n",
        "    std = ser.std()\n",
        "    return (ser - mean)/std\n",
        "\n",
        "for data_type in DATA_TYPES:\n",
        "    data_files = glob.glob(\"dataset/\" + data_type + \"/*.dat\")\n",
        "    for filename in data_files:\n",
        "        print(filename)\n",
        "        data = pd.read_table(filename, sep = \"\\s+\", header = None)\n",
        "\n",
        "        # clean the data\n",
        "        df = data[col_all].copy()\n",
        "        del data\n",
        "        gc.collect()\n",
        "        df.columns = col_all_name\n",
        "\n",
        "        #df.dropna(inplace=True)\n",
        "        means = np.nanmean(df, axis=0)\n",
        "\n",
        "        i = 0\n",
        "        for column in df.columns.values:\n",
        "            df[column] = df[column].fillna(means[i])\n",
        "            i += 1\n",
        "\n",
        "        df = df[(df[\"Locomotion\"] != 0) &\n",
        "                (df[\"LL_Right_Arm\"] != 0) &\n",
        "                (df[\"ML_Both_Arms\"] != 0)]\n",
        "\n",
        "        # remap the output as 0-indexing to make learning possible\n",
        "        df[\"Locomotion\"] = df[\"Locomotion\"].map({1: 0, 2: 1, 4: 2, 5: 3})\n",
        "        df[\"LL_Right_Arm\"] = df[\"LL_Right_Arm\"].map(\n",
        "            {401: 0, 402: 1, 403: 2, 404: 3, 405: 4, 406: 5, 407: 6, 408: 7,\n",
        "            409: 8, 410: 9, 411: 10, 412: 11, 413: 12})\n",
        "        df[\"ML_Both_Arms\"] = df[\"ML_Both_Arms\"].map(\n",
        "            {406516: 0, 406517: 1, 404516: 2, 404517: 3, 406520: 4, 404520: 5,\n",
        "            406505: 6, 404505: 7, 406519: 8, 404519: 9, 406511: 10, 404511: 11,\n",
        "            406508: 12, 404508: 13, 408512: 14, 407521: 15, 405506: 16})\n",
        "        \n",
        "        # apply normalization function\n",
        "        df[col_feature_name] = df[col_feature_name].apply(normalize, axis=0)\n",
        "        \n",
        "        file = filename.split(\"/\")[2][:-4]\n",
        "        df.to_csv(DATASET_CLEANED_DIR + \"/\" + data_type + \"/\" + file + \".csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4soD_F_W6Mx"
      },
      "source": [
        "# parse the data\n",
        "FILE_LENGTH = 30\n",
        "\n",
        "for data_type in DATA_TYPES:\n",
        "    data_files = glob.glob(DATASET_CLEANED_DIR + \"/\" + data_type + \"/*.csv\")\n",
        "    for filename in data_files:\n",
        "        print(filename)\n",
        "        i = 0\n",
        "        file = filename.split(\"/\")[2][:-4]\n",
        "        df = pd.read_csv(filename, header=0)\n",
        "\n",
        "        # cut the data for every specified length\n",
        "        df = df.assign(temp=df[\"LL_Right_Arm\"].diff().ne(0).cumsum())\n",
        "        for _, sub_df in df.groupby(\"temp\"):\n",
        "            sub_df.drop(columns=[\"temp\"], inplace=True)\n",
        "            sub_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            while len(sub_df) > FILE_LENGTH:\n",
        "                sub_df_cut = sub_df.loc[:FILE_LENGTH-1].copy()\n",
        "                sub_df_cut.to_csv(DATASET_PARSED_DIR + \"/\" + data_type + \"/\" +\n",
        "                                    file + \"-\" + str(i) + \".csv\", index=False)\n",
        "                sub_df = sub_df.loc[FILE_LENGTH:]\n",
        "                sub_df.reset_index(drop=True, inplace=True)\n",
        "                i += 1\n",
        "\n",
        "            # pad files that are less than specified length\n",
        "            padding = np.zeros([FILE_LENGTH-len(sub_df), len(sub_df.columns.values)])\n",
        "            padding_df = pd.DataFrame(padding, columns=col_all_name)\n",
        "            sub_df = padding_df.append(sub_df)\n",
        "\n",
        "            sub_df.to_csv(DATASET_PARSED_DIR + \"/\" + data_type + \"/\" + file + \"-\" +\n",
        "                        str(i) + \".csv\", index=False)\n",
        "\n",
        "            i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qww--NGA3D7w"
      },
      "source": [
        "# check the average length of each partition\n",
        "# check for possible NaN files\n",
        "# check for target distribution\n",
        "file_lengths = []\n",
        "targets = []\n",
        "\n",
        "for data_type in DATA_TYPES:\n",
        "    data_files = glob.glob(DATASET_PARSED_DIR + \"/\" + data_type + \"/*.csv\")\n",
        "    for filename in data_files:\n",
        "        df = pd.read_csv(filename, header=0)\n",
        "        file_length = len(df)\n",
        "        file_lengths.append(file_length)\n",
        "        targets.append(int(df.loc[FILE_LENGTH-1, \"LL_Right_Arm\"]))\n",
        "        array_sum = np.sum(df.to_numpy())\n",
        "        if np.isnan(array_sum):\n",
        "            print(\"NaN files: \" + filename)\n",
        "\n",
        "print(\"Min length: \" + str(np.min(file_lengths)))\n",
        "print(\"Max length: \" + str(np.max(file_lengths)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DiDK6UoBtZL"
      },
      "source": [
        "len(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgoFGDF2BQOi"
      },
      "source": [
        "def CountFrequency(my_list): \n",
        "    # Creating an empty dictionary  \n",
        "    freq = {} \n",
        "    for item in my_list: \n",
        "        if (item in freq): \n",
        "            freq[item] += 1\n",
        "        else: \n",
        "            freq[item] = 1\n",
        "  \n",
        "    for key, value in freq.items(): \n",
        "        print (\"% d : % d\"%(key, value)) \n",
        "\n",
        "CountFrequency(targets) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmdqCGpgBm9A"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}