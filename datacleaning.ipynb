{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datacleaning.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghifarahadian/concurrent-activity-recognition/blob/master/datacleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPHzniw2TX_4"
      },
      "source": [
        "# this is a classic usage for google colab\n",
        "\n",
        "import sys\n",
        "import os.path as osp\n",
        "import os\n",
        "import gc\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT = osp.join('/content', 'drive', 'My Drive', 'concurrent-activity-recognition')\n",
        "os.chdir(ROOT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvRzpABgTp-w"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTZndHicTwiv"
      },
      "source": [
        "# data column constant\n",
        "\n",
        "col_right_wrist = [22, 23, 24]\n",
        "col_right_wrist_name = [\"right_wrist_x\", \"right_wrist_y\", \"right_wrist_z\"]\n",
        "col_left_wrist = [31, 32, 33]\n",
        "col_left_wrist_name = [\"left_wrist_x\", \"left_wrist_y\", \"left_wrist_z\"]\n",
        "col_right_hand = [34, 35, 36]\n",
        "col_right_hand_name = [\"right_hand_x\", \"right_hand_y\", \"right_hand_z\"]\n",
        "col_left_hand = [13, 14, 15]\n",
        "col_left_hand_name = [\"left_hand_x\", \"left_hand_y\", \"left_hand_z\"]\n",
        "col_hip = [4, 5, 6]\n",
        "col_hip_name = [\"hip_x\", \"hip_y\", \"hip_z\"]\n",
        "col_label = [243, 247, 249]\n",
        "col_label_name = [\"Locomotion\", \"LL_Right_Arm\", \"ML_Both_Arms\"]\n",
        "\n",
        "col_all = []\n",
        "for col in [col_hip, col_left_hand, col_right_wrist, col_left_wrist,\n",
        "            col_right_hand, col_label]:\n",
        "    col_all.extend(col)\n",
        "\n",
        "col_all_name = []\n",
        "for col in [col_hip_name, col_left_hand_name, col_right_wrist_name,\n",
        "            col_left_wrist_name, col_right_hand_name, col_label_name]:\n",
        "    col_all_name.extend(col)\n",
        "\n",
        "col_feature_name = []\n",
        "for col in [col_hip_name, col_left_hand_name, col_right_wrist_name,\n",
        "            col_left_wrist_name, col_right_hand_name]:\n",
        "    col_feature_name.extend(col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJhgBxdVAQD"
      },
      "source": [
        "# normalizing function\n",
        "\n",
        "def normalize(ser):\n",
        "    mean = ser.mean()\n",
        "    std = ser.std()\n",
        "    return (ser - mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6NeygvZVD4A"
      },
      "source": [
        "# clean the data, both train data and test data\n",
        "data_types = [\"train\", \"test\"]\n",
        "\n",
        "for data_type in data_types:\n",
        "    data_files = glob.glob(\"dataset/\" + data_type + \"/*.dat\")\n",
        "    for filename in data_files:\n",
        "        print(filename)\n",
        "        data = pd.read_table(filename, sep = \"\\s+\", header = None)\n",
        "\n",
        "        # clean the data\n",
        "        df = data[col_all].copy()\n",
        "        del data\n",
        "        gc.collect()\n",
        "        df.columns = col_all_name\n",
        "\n",
        "        df.dropna(inplace=True)\n",
        "        df = df[(df[\"Locomotion\"] != 0) &\n",
        "                (df[\"LL_Right_Arm\"] != 0) &\n",
        "                (df[\"ML_Both_Arms\"] != 0)]\n",
        "\n",
        "        # remap the output as 0-indexing to make learning possible\n",
        "        df[\"Locomotion\"] = df[\"Locomotion\"].map({1: 0, 2: 1, 4: 2, 5: 3})\n",
        "        df[\"LL_Right_Arm\"] = df[\"LL_Right_Arm\"].map(\n",
        "            {401: 0, 402: 1, 403: 2, 404: 3, 405: 4, 406: 5, 407: 6, 408: 7,\n",
        "            409: 8, 410: 9, 411: 10, 412: 11, 413: 12})\n",
        "        df[\"ML_Both_Arms\"] = df[\"ML_Both_Arms\"].map(\n",
        "            {406516: 0, 406517: 1, 404516: 2, 404517: 3, 406520: 4, 404520: 5,\n",
        "            406505: 6, 404505: 7, 406519: 8, 404519: 9, 406511: 10, 404511: 11,\n",
        "            406508: 12, 404508: 13, 408512: 14, 407521: 15, 405506: 16})\n",
        "        \n",
        "        #df[col_feature_name] = df[col_feature_name].apply(normalize, axis=1)\n",
        "        \n",
        "        file = filename.split(\"/\")[2][:-4]\n",
        "        df.to_csv(\"dataset_cleaned/\" + data_type + \"/\" + file + \".csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4soD_F_W6Mx"
      },
      "source": [
        "# parse the data\n",
        "data_types = [\"train\", \"test\"]\n",
        "\n",
        "for data_type in data_types:\n",
        "    data_files = glob.glob(\"dataset_cleaned/\" + data_type + \"/*.csv\")\n",
        "    for filename in data_files:\n",
        "        i = 0\n",
        "        file = filename.split(\"/\")[2][:-4]\n",
        "        print(filename)\n",
        "        print(file)\n",
        "        df = pd.read_csv(filename, header=0)\n",
        "\n",
        "        df = df.assign(temp=df[\"Locomotion\"].diff().ne(0).cumsum())\n",
        "\n",
        "        for _, sub_df in df.groupby(\"temp\"):\n",
        "            sub_df.drop(columns=[\"temp\"], inplace=True)\n",
        "            sub_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "            while len(sub_df) > 30:\n",
        "                sub_df_cut = sub_df.loc[0:29].copy()\n",
        "                #print(sub_df_cut)\n",
        "                sub_df_cut[col_feature_name] = sub_df_cut[col_feature_name].apply(normalize, axis=0)\n",
        "                #print(sub_df_cut)\n",
        "                #break\n",
        "                sub_df_cut.to_csv(\"dataset_parsed/\" + data_type + \"/\" +\n",
        "                                    file + \"-\" + str(i) + \".csv\", index=False)\n",
        "                sub_df = sub_df.loc[30:]\n",
        "                sub_df.reset_index(drop=True, inplace=True)\n",
        "                i += 1\n",
        "            #break\n",
        "            # normalize first, pad later\n",
        "            sub_df[col_feature_name] = sub_df[col_feature_name].apply(normalize, axis=0)\n",
        "\n",
        "            # pad here\n",
        "            padding = np.zeros([30-len(sub_df), 18])\n",
        "            padding_df = pd.DataFrame(padding, columns=col_all_name)\n",
        "            sub_df = padding_df.append(sub_df)\n",
        "\n",
        "            sub_df.to_csv(\"dataset_parsed/\" + data_type + \"/\" + file + \"-\" +\n",
        "                        str(i) + \".csv\", index=False)\n",
        "\n",
        "            i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t5nr8Iz19wf"
      },
      "source": [
        "temp = pd.DataFrame([[1, 2], [3, 4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGzyIUas2Cnr"
      },
      "source": [
        "temp2 = temp.apply(normalize, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKUyfUWx2IL1"
      },
      "source": [
        "temp2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDCFfPBpEN4y"
      },
      "source": [
        "test = np.zeros([100-len(sub_df), 18])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DQ6nM4rESpo"
      },
      "source": [
        "test_df = pd.DataFrame(test, columns=col_all_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKuh_Xt3EZ1e"
      },
      "source": [
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9vuVk9yFzIw"
      },
      "source": [
        "sub_df = test_df.append(sub_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvvVLVUj7E-I"
      },
      "source": [
        "drop_index = [i for i in range(0, 10)]\n",
        "\n",
        "df_new1 = sub_df.loc[:80, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE7ITV4K8_Lz"
      },
      "source": [
        "sub_df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA6Avpbr9ghV"
      },
      "source": [
        "sub_df = sub_df[10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEA2kft-9nK9"
      },
      "source": [
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbye4dnsKhmd"
      },
      "source": [
        "# check the average length of each partition\n",
        "\n",
        "data_types = [\"train\", \"test\"]\n",
        "\n",
        "targets = []\n",
        "\n",
        "for data_type in data_types:\n",
        "    data_files = glob.glob(\"dataset_parsed/\" + data_type + \"/*.csv\")\n",
        "    for filename in data_files:\n",
        "        print(filename)\n",
        "        target = pd.read_csv(filename, header=0).iloc[29, 15]\n",
        "        targets.append(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gdidk_JLMKh"
      },
      "source": [
        "targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAf6HEfgKzgB"
      },
      "source": [
        "from itertools import groupby\n",
        "[len(list(group)) for key, group in groupby(targets)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qww--NGA3D7w"
      },
      "source": [
        "# check the average length of each partition\n",
        "\n",
        "data_types = [\"train\", \"test\"]\n",
        "\n",
        "file_lengths = []\n",
        "\n",
        "for data_type in data_types:\n",
        "    data_files = glob.glob(\"dataset_parsed/\" + data_type + \"/*.csv\")\n",
        "    for filename in data_files:\n",
        "        print(filename)\n",
        "        file_length = len(pd.read_csv(filename, header=0))\n",
        "        file_lengths.append(file_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-OE5Ewt-U87"
      },
      "source": [
        "np.sum(file_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQdMYPMt-7p0"
      },
      "source": [
        "len(file_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0DqBpy55gnP"
      },
      "source": [
        "temp = pd.read_csv(\"dataset_parsed/train/S3-ADL1-8.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmBBdMoXB7bL"
      },
      "source": [
        "temp.loc[99:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWBXH8mB3szf"
      },
      "source": [
        "p31 = np.asarray(file_lengths)\n",
        "(p31 < 100).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l02DpgFJBQib"
      },
      "source": [
        "np.min(file_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edvgoaNN4Jb4"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vXPUFJt4160"
      },
      "source": [
        "sns.displot(file_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNPt12zphD2w"
      },
      "source": [
        "!pip install --upgrade seaborn"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}