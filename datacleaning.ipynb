{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/concurrent-activity-recognition\n"
     ]
    }
   ],
   "source": [
    "# this is a classic usage for AWS\n",
    "\n",
    "import sys\n",
    "import os\n",
    "ROOT = os.path.join('/home', 'ubuntu', 'concurrent-activity-recognition')\n",
    "os.chdir(ROOT)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data column constant\n",
    "USE_FEATURES = \"few\"\n",
    "DATASET_CLEANED_DIR = \"dataset_cleaned_few_features\"\n",
    "DATASET_PARSED_DIR = \"dataset_parsed_few_features_label0_label1\"\n",
    "TARGET_LABEL = \"LL_Right_Arm\"\n",
    "DATA_TYPES = [\"train\", \"test\"]\n",
    "\n",
    "if USE_FEATURES == \"few\":\n",
    "    # This is the list of features that are retrievable from\n",
    "    # fitbit and phone\n",
    "    col_right_wrist = [22, 23, 24]\n",
    "    col_right_wrist_name = [\"right_wrist_x\", \"right_wrist_y\", \"right_wrist_z\"]\n",
    "    col_left_wrist = [31, 32, 33]\n",
    "    col_left_wrist_name = [\"left_wrist_x\", \"left_wrist_y\", \"left_wrist_z\"]\n",
    "    col_right_hand = [34, 35, 36]\n",
    "    col_right_hand_name = [\"right_hand_x\", \"right_hand_y\", \"right_hand_z\"]\n",
    "    col_left_hand = [13, 14, 15]\n",
    "    col_left_hand_name = [\"left_hand_x\", \"left_hand_y\", \"left_hand_z\"]\n",
    "    col_hip = [4, 5, 6]\n",
    "    col_hip_name = [\"hip_x\", \"hip_y\", \"hip_z\"]\n",
    "    col_label = [243, 247, 249]\n",
    "    col_label_name = [\"Locomotion\", \"LL_Right_Arm\", \"ML_Both_Arms\"]\n",
    "\n",
    "    col_all = []\n",
    "    for col in [col_hip, col_left_hand, col_right_wrist, col_left_wrist,\n",
    "                col_right_hand, col_label]:\n",
    "        col_all.extend(col)\n",
    "\n",
    "    col_all_name = []\n",
    "    for col in [col_hip_name, col_left_hand_name, col_right_wrist_name,\n",
    "                col_left_wrist_name, col_right_hand_name, col_label_name]:\n",
    "        col_all_name.extend(col)\n",
    "\n",
    "    col_feature_name = []\n",
    "    for col in [col_hip_name, col_left_hand_name, col_right_wrist_name,\n",
    "                col_left_wrist_name, col_right_hand_name]:\n",
    "        col_feature_name.extend(col)\n",
    "elif USE_FEATURES == \"all\":\n",
    "    # This is the list of all accelerometer data from sensors placed on body\n",
    "    col_all = [i for i in range(1, 37)]\n",
    "    col_all.extend([243, 247, 249])\n",
    "\n",
    "    feature_list = [\"RKNUp\", \"Hip\", \"LUAUp\", \"RUADn\", \"LH\", \"Back\", \"RKNDn\",\n",
    "                    \"RWR\", \"RUAUp\", \"LUADn\", \"LWR\", \"RH\"]\n",
    "    axis_list = [\"X\", \"Y\", \"Z\"]\n",
    "    col_feature_name = [f + \"_\" + a\n",
    "                        for f in feature_list\n",
    "                        for a in axis_list]\n",
    "\n",
    "    col_all_name = col_feature_name.copy()\n",
    "    col_all_name.extend([\"Locomotion\", \"LL_Right_Arm\", \"ML_Both_Arms\"])\n",
    "else:\n",
    "    raise Exception(\"USE_FEATURES is not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train/S3-Drill.dat\n",
      "dataset/train/S3-ADL3.dat\n",
      "dataset/train/S3-ADL4.dat\n",
      "dataset/train/S3-ADL2.dat\n",
      "dataset/train/S3-ADL1.dat\n",
      "dataset/test/S3-ADL5.dat\n"
     ]
    }
   ],
   "source": [
    "# clean the data, both train data and test data\n",
    "\n",
    "# normalizing function\n",
    "def normalize(ser):\n",
    "    mean = ser.mean()\n",
    "    std = ser.std()\n",
    "    return (ser - mean)/std\n",
    "\n",
    "for data_type in DATA_TYPES:\n",
    "    data_files = glob.glob(\"dataset/\" + data_type + \"/*.dat\")\n",
    "    for filename in data_files:\n",
    "        print(filename)\n",
    "        data = pd.read_table(filename, sep = \"\\s+\", header = None)\n",
    "\n",
    "        # clean the data\n",
    "        df = data[col_all].copy()\n",
    "        del data\n",
    "        df.columns = col_all_name\n",
    "\n",
    "        #df.dropna(inplace=True)\n",
    "        means = np.nanmean(df, axis=0)\n",
    "\n",
    "        i = 0\n",
    "        for column in df.columns.values:\n",
    "            df[column] = df[column].fillna(means[i])\n",
    "            i += 1\n",
    "\n",
    "        df = df[(df[\"Locomotion\"] != 0) &\n",
    "                (df[\"LL_Right_Arm\"] != 0) &\n",
    "                (df[\"ML_Both_Arms\"] != 0)]\n",
    "\n",
    "        # remap the output as 0-indexing to make learning possible\n",
    "        df[\"Locomotion\"] = df[\"Locomotion\"].map({1: 0, 2: 1, 4: 2, 5: 3})\n",
    "        df[\"LL_Right_Arm\"] = df[\"LL_Right_Arm\"].map(\n",
    "            {401: 0, 402: 1, 403: 2, 404: 3, 405: 4, 406: 5, 407: 6, 408: 7,\n",
    "            409: 8, 410: 9, 411: 10, 412: 11, 413: 12})\n",
    "        df[\"ML_Both_Arms\"] = df[\"ML_Both_Arms\"].map(\n",
    "            {406516: 0, 406517: 1, 404516: 2, 404517: 3, 406520: 4, 404520: 5,\n",
    "            406505: 6, 404505: 7, 406519: 8, 404519: 9, 406511: 10, 404511: 11,\n",
    "            406508: 12, 404508: 13, 408512: 14, 407521: 15, 405506: 16})\n",
    "        \n",
    "        # apply normalization function\n",
    "        df[col_feature_name] = df[col_feature_name].apply(normalize, axis=0)\n",
    "        \n",
    "        file = filename.split(\"/\")[2][:-4]\n",
    "        df.to_csv(DATASET_CLEANED_DIR + \"/\" + data_type + \"/\" + file + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_cleaned_few_features/train/S3-Drill.csv\n",
      "dataset_cleaned_few_features/train/S3-ADL3.csv\n",
      "dataset_cleaned_few_features/train/S3-ADL4.csv\n",
      "dataset_cleaned_few_features/train/S3-ADL1.csv\n",
      "dataset_cleaned_few_features/train/S3-ADL2.csv\n",
      "dataset_cleaned_few_features/test/S3-ADL5.csv\n"
     ]
    }
   ],
   "source": [
    "# parse the data\n",
    "FILE_LENGTH = 30\n",
    "\n",
    "for data_type in DATA_TYPES:\n",
    "    data_files = glob.glob(DATASET_CLEANED_DIR + \"/\" + data_type + \"/*.csv\")\n",
    "    for filename in data_files:\n",
    "        print(filename)\n",
    "        i = 0\n",
    "        file = filename.split(\"/\")[2][:-4]\n",
    "        df = pd.read_csv(filename, header=0)\n",
    "\n",
    "#         # cut the data for every specified length\n",
    "#         df = df.assign(temp=df[TARGET_LABEL].diff().ne(0).cumsum())\n",
    "        \n",
    "        # cut data for multiple labels\n",
    "        df = df.assign(label01 = (df[\"Locomotion\"].astype(str) + df[\"LL_Right_Arm\"].astype(str)).astype(int))\n",
    "        df = df.assign(temp = df[\"label01\"].diff().ne(0).cumsum())\n",
    "        df.drop(\"label01\", axis=1, inplace=True)\n",
    "        \n",
    "        for _, sub_df in df.groupby(\"temp\"):\n",
    "            sub_df.drop(columns=[\"temp\"], inplace=True)\n",
    "            sub_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            while len(sub_df) > FILE_LENGTH:\n",
    "                sub_df_cut = sub_df.loc[:FILE_LENGTH-1].copy()\n",
    "                sub_df_cut.to_csv(DATASET_PARSED_DIR + \"/\" + data_type + \"/\" +\n",
    "                                    file + \"-\" + str(i) + \".csv\", index=False)\n",
    "                sub_df = sub_df.loc[FILE_LENGTH:]\n",
    "                sub_df.reset_index(drop=True, inplace=True)\n",
    "                i += 1\n",
    "\n",
    "            # pad files that are less than specified length\n",
    "            padding = np.zeros([FILE_LENGTH-len(sub_df), len(sub_df.columns.values)])\n",
    "            padding_df = pd.DataFrame(padding, columns=col_all_name)\n",
    "            sub_df = padding_df.append(sub_df)\n",
    "\n",
    "            sub_df.to_csv(DATASET_PARSED_DIR + \"/\" + data_type + \"/\" + file + \"-\" +\n",
    "                        str(i) + \".csv\", index=False)\n",
    "\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 30\n",
      "Max length: 30\n"
     ]
    }
   ],
   "source": [
    "# check the average length of each partition\n",
    "# check for possible NaN files\n",
    "# check for target distribution\n",
    "file_lengths = []\n",
    "targets = []\n",
    "\n",
    "for data_type in DATA_TYPES:\n",
    "    data_files = glob.glob(DATASET_PARSED_DIR + \"/\" + data_type + \"/*.csv\")\n",
    "    for filename in data_files:\n",
    "        df = pd.read_csv(filename, header=0)\n",
    "        file_length = len(df)\n",
    "        file_lengths.append(file_length)\n",
    "        targets.append(int(df.loc[FILE_LENGTH-1, TARGET_LABEL]))\n",
    "        array_sum = np.sum(df.to_numpy())\n",
    "        if np.isnan(array_sum):\n",
    "            print(\"NaN files: \" + filename)\n",
    "\n",
    "print(\"Min length: \" + str(np.min(file_lengths)))\n",
    "print(\"Max length: \" + str(np.max(file_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 :  589\n",
      " 12 :  475\n",
      " 7 :  118\n",
      " 2 :  65\n",
      " 5 :  347\n",
      " 11 :  462\n",
      " 3 :  346\n",
      " 6 :  150\n",
      " 0 :  57\n"
     ]
    }
   ],
   "source": [
    "def CountFrequency(my_list): \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "  \n",
    "    for key, value in freq.items(): \n",
    "        print (\"% d : % d\"%(key, value)) \n",
    "\n",
    "CountFrequency(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2609"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p36)",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
